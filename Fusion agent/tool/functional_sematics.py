from openai import OpenAI
import pandas as pd
import google.generativeai as genai

import time
import os
import random
# # C·∫•u h√¨nh Gemini API
# genai.configure(api_key="AIzaSyCAz4jOl918hOprkoGNrStIcEJhW1ydQLw")
# model = genai.GenerativeModel("gemini-1.5-flash")


# # C·∫•u h√¨nh OpenAI API
# client = OpenAI(api_key="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")  # Thay b·∫±ng API Key th·∫≠t c·ªßa b·∫°n)  # Thay API key c·ªßa b·∫°n

# MODEL_NAME = "gpt-3.5-turbo"  # ho·∫∑c "gpt-4" n·∫øu b·∫°n c√≥ quy·ªÅn

# ƒê·ªçc file Excel g·ªëc
input_file = "../solidity_code_with_labels-3k.xlsx"
df = pd.read_excel(input_file)

if "functional_semantics" not in df.columns:
    df["functional_semantics"] = ""

def build_prompt(code):
    return f"""
What is the purpose of the following Solidity code? Please summarize the answer in one sentence starting with:
‚ÄúAbstract purpose:‚Äù.

Then list the key functionalities in the following format:
‚ÄúDetail behaviors:
1. ...
2. ...
3. ...‚Äù

Here is the Solidity code:
{code}
"""


# # === H√ÄM G·ªåI GEMINI C√ì RETRY ===
# def safe_generate_content(prompt, retries=3):
#     for attempt in range(retries):
#         try:
#             response = model.generate_content(prompt)
#             return response.text.strip()
#         except Exception as e:
#             print(f"‚ö†Ô∏è L·ªói (l·∫ßn {attempt+1}): {e}")
#             if "quota" in str(e).lower() or "rate" in str(e).lower():
#                 wait = random.uniform(30, 60)
#                 print(f"‚è≥ ƒêang ch·ªù {wait:.1f}s tr∆∞·ªõc khi th·ª≠ l·∫°i...")
#                 time.sleep(wait)
#             else:
#                 break
#     return "ERROR"

# # === X·ª¨ L√ù T·ª™NG D√íNG ===
# for idx, row in df.iterrows():
#     if pd.notna(row["functional_semantics"]) and row["functional_semantics"].strip() != "":
#         print(f"‚ö†Ô∏è [{idx+1}] ƒê√£ c√≥, b·ªè qua.")
#         continue

#     code = row["code"]
#     prompt = build_prompt(code)
#     output = safe_generate_content(prompt)
#     df.at[idx, "functional_semantics"] = output

#     print(f"‚úÖ [{idx+1}/{len(df)}] ƒê√£ x·ª≠ l√Ω xong.")

#     # === Ch·ªù gi·ªØa c√°c request ===
#     time.sleep(random.uniform(5.0, 8.0))

#     # === Ghi t·∫°m m·ªói 10 d√≤ng ===
#     if (idx + 1) % 10 == 0:
#         df.to_excel(input_file, index=False)
#         print("üíæ ƒê√£ l∆∞u t·∫°m v√†o file Excel.")

# # === GHI FILE CU·ªêI C√ôNG ===
# df.to_excel(input_file, index=False)
# print(f"‚úÖ ƒê√£ ghi ho√†n t·∫•t v√†o file: {input_file}")




# # ‚úÖ H√ÄM G·ªåI OPENAI C√ì RETRY
# def safe_generate_content(prompt, retries=3):
#     for attempt in range(retries):
#         try:
#             completion = client.chat.completions.create(
#                 model=MODEL_NAME,
#                 messages=[
#                     {"role": "system", "content": "You are a specialist in Smart Contract analyzing. Talk like an expert in Smart Contract."},
#                     {"role": "user", "content": prompt}
#                 ],
#                 temperature=0.5
#             )
#             return completion.choices[0].message.content.strip()
#         except Exception as e:
#             print(f"‚ö†Ô∏è L·ªói (l·∫ßn {attempt+1}): {e}")
#             if "rate" in str(e).lower() or "quota" in str(e).lower():
#                 wait = random.uniform(30, 60)
#                 print(f"‚è≥ ƒêang ch·ªù {wait:.1f}s tr∆∞·ªõc khi th·ª≠ l·∫°i...")
#                 time.sleep(wait)
#             else:
#                 break
#     return "ERROR"

# # ‚úÖ X·ª¨ L√ù T·ª™NG D√íNG
# for idx, row in df.iterrows():
#     if pd.notna(row["functional_semantics"]) and row["functional_semantics"].strip() != "":
#         print(f"‚ö†Ô∏è [{idx+1}] ƒê√£ c√≥, b·ªè qua.")
#         continue

#     code = row["code"]
#     prompt = build_prompt(code)
#     output = safe_generate_content(prompt)
#     df.at[idx, "functional_semantics"] = output

#     print(f"‚úÖ [{idx+1}/{len(df)}] ƒê√£ x·ª≠ l√Ω xong.")

#     # ‚è≥ Ch·ªù gi·ªØa c√°c request
#     time.sleep(random.uniform(5.0, 8.0))

#     # üíæ Ghi t·∫°m m·ªói 10 d√≤ng
#     if (idx + 1) % 10 == 0:
#         df.to_excel(input_file, index=False)
#         print("üíæ ƒê√£ l∆∞u t·∫°m v√†o file Excel.")

# # ‚úÖ GHI FILE CU·ªêI C√ôNG
# df.to_excel(input_file, index=False)
# print(f"‚úÖ ƒê√£ ghi ho√†n t·∫•t v√†o file: {input_file}")



###OLLAMA

import requests

def safe_generate_content(prompt, retries=3):
    for attempt in range(retries):
        try:
            response = requests.post(
                "http://localhost:11434/api/generate",
                json={
                    "model": "gemma2:9b",  # Model b·∫°n ƒë√£ pull b·∫±ng `ollama run gemma2`
                    "prompt": prompt,
                    "stream": False
                }
            )
            result = response.json()
            return result["response"].strip()
        except Exception as e:
            print(f"‚ö†Ô∏è L·ªói (l·∫ßn {attempt+1}): {e}")
            time.sleep(random.uniform(5, 10))
    return "ERROR"



for idx, row in df.iterrows():
    if pd.notna(row["functional_semantics"]) and row["functional_semantics"].strip() != "":
        print(f"‚ö†Ô∏è [{idx+1}] ƒê√£ c√≥, b·ªè qua.")
        continue

    code = row["code"]
    prompt = build_prompt(code)
    output = safe_generate_content(prompt)
    df.at[idx, "functional_semantics"] = output

    print(f"‚úÖ [{idx+1}/{len(df)}] ƒê√£ x·ª≠ l√Ω xong.")
    time.sleep(random.uniform(5.0, 8.0))

    if (idx + 1) % 10 == 0:
        df.to_excel(input_file, index=False)
        print("üíæ ƒê√£ l∆∞u t·∫°m v√†o file Excel.")

df.to_excel(input_file, index=False)
print(f"‚úÖ ƒê√£ ghi ho√†n t·∫•t v√†o file: {input_file}")

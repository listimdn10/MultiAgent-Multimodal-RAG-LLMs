# agent_fusion.py (Version: ƒê√£ s·ª≠a l·ªói NoneType v√† L·∫∑p v√¥ h·∫°n)
import json, pickle, torch, numpy as np
import torch.nn as nn
from pydantic import BaseModel, Field
from typing import Type
from crewai import Agent, Task, LLM
from crewai.tools import BaseTool
import joblib # Import joblib ·ªü ƒë·∫ßu
from tools.fusion_model import EarlyFusionModel # ƒê·∫£m b·∫£o b·∫°n c√≥ file n√†y

# ===========================
# --- HELPER V√Ä GLOBAL READ (M·ªöI) ---
# ===========================

def safe_read_json(path):
    try:
        with open(path, "r", encoding="utf-8") as f:
            return json.load(f)
    except Exception as e:
        print(f"‚ö†Ô∏è Could not read JSON file {path}: {e}")
        return {}

def safe_read_text(path):
    """H√†m helper ƒë·ªÉ ƒë·ªçc file text m·ªôt c√°ch an to√†n."""
    try:
        with open(path, "r", encoding="utf-8") as f:
            return f.read()
    except Exception as e:
        print(f"‚ö†Ô∏è Could not read text file {path}: {e}")
        return ""

# ‚úÖ ƒê·ªçc file source code M·ªòT L·∫¶N ·ªü global, gi·ªëng h·ªát consensus_agent.py
SOURCE_CODE_PATH = "contracts/sample.sol"
SOURCE_CODE_CONTENT = safe_read_text(SOURCE_CODE_PATH)

# ‚úÖ ƒê·ªçc file embedding M·ªòT L·∫¶N ·ªü global
SOURCE_EMBEDDING_PATH = "parser_output.json"
SOURCE_EMBEDDING_CONTENT = safe_read_json(SOURCE_EMBEDDING_PATH)

if SOURCE_CODE_CONTENT:
    print(f"‚úÖ ƒê√£ ƒë·ªçc th√†nh c√¥ng file source code global: {SOURCE_CODE_PATH}")
else:
    print(f"‚ùå L·ªñI NGHI√äM TR·ªåNG: Kh√¥ng th·ªÉ ƒë·ªçc file {SOURCE_CODE_PATH} ·ªü global.")

if SOURCE_EMBEDDING_CONTENT:
    print(f"‚úÖ ƒê√£ ƒë·ªçc th√†nh c√¥ng file embedding global: {SOURCE_EMBEDDING_PATH}")
else:
    print(f"‚ùå L·ªñI NGHI√äM TR·ªåNG: Kh√¥ng th·ªÉ ƒë·ªçc file {SOURCE_EMBEDDING_PATH} ·ªü global.")


# # ==== MLP MODEL (Gi·ªØ nguy√™n) ====
# class MLP(nn.Module):
#     def __init__(self, input_size, num_classes):
#         super(MLP, self).__init__()
#         self.model = nn.Sequential(
#             nn.Linear(input_size, 256),
#             nn.ReLU(),
#             nn.Dropout(0.2),
#             nn.Linear(256, 128),
#             nn.ReLU(),
#             nn.Dropout(0.2),
#             nn.Linear(128, num_classes)
#         )

#     def forward(self, x):
#         return self.model(x)


# ==== TOOL (C·∫¨P NH·∫¨T) ====

# ‚úÖ THAY ƒê·ªîI 1: T·∫°o m·ªôt class Pydantic R·ªóng
class NoArgs(BaseModel):
    """Kh√¥ng nh·∫≠n b·∫•t k·ª≥ ƒë·ªëi s·ªë n√†o."""
    pass

class EmbeddingPredictorTool(BaseTool):
    name: str = "Fusion Transformer Vulnerability Predictor and Line Finder"
    description: str = "Predicts security vulnerability from 3 embedding types using Fusion Transformer AND uses an LLM to find the vulnerable line."
    
    # ‚úÖ THAY ƒê·ªîI 1: S·ª≠ d·ª•ng class Pydantic r·ªóng thay v√¨ None
    args_schema: Type[BaseModel] = NoArgs 
    _llm: LLM = None

    def __init__(self, llm: LLM, **kwargs):
        super().__init__(**kwargs)
        self._llm = llm

    # -----------------------------------------
    # LOAD FUSION TRANSFORMER MODEL (NEW)
    # -----------------------------------------
    def _load_model(self, d_scode, d_fsem, d_cfg):
        # Load label encoder
        self._label_encoder = joblib.load("tools/early_fusion_label_encoder.pkl")
        num_classes = len(self._label_encoder.classes_)

        # Build model
        self._model = EarlyFusionModel(
            d_sc=d_scode,
            d_fs=d_fsem,
            d_cfg=d_cfg,
            n_classes=num_classes
        )
        # Load weights (S·ª≠ d·ª•ng ƒë∆∞·ªùng d·∫´n c·ªßa b·∫°n)
        state = torch.load("tools/early_fusion.pth", map_location="cpu")
        self._model.load_state_dict(state)
        self._model.eval()

    # H√†m _call_llm an to√†n (Gi·ªØ nguy√™n)
    def _call_llm(self, prompt: str):
        if self._llm is None:
            raise RuntimeError("‚ùå No LLM provided to EmbeddingPredictorTool.")
        errors = []
        for method in ["invoke", "generate", "call"]:
            try:
                fn = getattr(self._llm, method, None)
                if callable(fn):
                    resp = fn(prompt)
                    if isinstance(resp, str):
                        return resp
                    if hasattr(resp, "content"):
                        return resp.content
                    if hasattr(resp, "text"):
                        return resp.text
                    return str(resp)
            except Exception as e:
                errors.append(f"{method} failed: {e}")
        raise RuntimeError("All LLM invocation attempts failed: " + " | ".join(errors))

    # ‚úÖ S·ª≠a h√†m _run, kh√¥ng nh·∫≠n 'path' n·ªØa (b·∫°n ƒë√£ l√†m ƒë√∫ng)
    def _run(self) -> str:
        
        # --- 1. Ph·∫ßn Logic Model ---
        
        # L·∫•y data t·ª´ bi·∫øn global
        data = SOURCE_EMBEDDING_CONTENT
        if not data:
             print(f"‚ùå Error: Bi·∫øn global 'SOURCE_EMBEDDING_CONTENT' b·ªã r·ªóng.")
             return "Error: Embedding data was not loaded globally."

        # ---- Extract 3 embedding vectors ----
        def extract(key):
            v = data.get(key, [])
            if isinstance(v, list) and len(v) > 0:
                v = v[0] if isinstance(v[0], list) else v
            return np.array(v, dtype=np.float32)

        fsem_vec  = extract("functional_semantic_embeddings")
        scode_vec = extract("code_embeddings")
        cfg_vec   = extract("cfg_embeddings")
        
        # Ki·ªÉm tra n·∫øu vector r·ªóng
        if fsem_vec.size == 0 or scode_vec.size == 0 or cfg_vec.size == 0:
            error_msg = "M·ªôt ho·∫∑c nhi·ªÅu vector embedding b·ªã r·ªóng. Kh√¥ng th·ªÉ d·ª± ƒëo√°n."
            print(f"‚ùå {error_msg}")
            return error_msg

        # dimensions
        d_fsem  = fsem_vec.shape[0]
        d_scode = scode_vec.shape[0]
        d_cfg   = cfg_vec.shape[0]

        # ---- Load model once ----
        if not hasattr(self, "_model"):
            self._load_model(d_scode, d_fsem, d_cfg)

        # ---- Convert to tensor ----
        fs = torch.tensor(fsem_vec, dtype=torch.float32).unsqueeze(0)
        sc = torch.tensor(scode_vec, dtype=torch.float32).unsqueeze(0)
        cf = torch.tensor(cfg_vec,  dtype=torch.float32).unsqueeze(0)

        # ---- Predict ----
        self._model.eval()  # Ensure model is in eval mode
        with torch.no_grad():
            logits = self._model(sc, fs, cf)
            probs = torch.softmax(logits, dim=1)
            pred_idx = torch.argmax(probs, dim=1).item()

        label = self._label_encoder.inverse_transform([pred_idx])[0]
        confidence = float(probs[0, pred_idx])

        print(f"‚úÖ Fusion Predicted: {label} (Confidence: {confidence:.2%})")

        # --- 2. Ph·∫ßn Logic LLM ---
        
        # L·∫•y source code t·ª´ bi·∫øn GLOBAL
        source_code = SOURCE_CODE_CONTENT 
        
        if not source_code:
            print(f"‚ùå Error: Bi·∫øn global 'SOURCE_CODE_CONTENT' b·ªã r·ªóng.")
            return "Error: Source code was not loaded globally."

        prompt = f"""
        Here is a Solidity smart contract:
        ```solidity
        {source_code}
        ```
        An analysis model has predicted that this code contains a vulnerability of type: **{label}**.

        Your task is to analyze the source code and identify the specific line number(s) that are most likely responsible for this **{label}** vulnerability.

        Respond ONLY with the line number(s). For example: "Line 42" or "Lines 10-15". If you are unsure, respond "Unknown".
        """

        print(f"ü§ñ Calling LLM to find line number for {label}...")
        try:
            llm_response = self._call_llm(prompt) 
            predicted_line = llm_response.strip().replace("`", "")
        except Exception as e:
            print(f"‚ùå Error calling LLM: {e}")
            predicted_line = "Error calling LLM"
        
        print(f"‚úÖ LLM Predicted Line: {predicted_line}")

        # --- 3. L∆∞u k·∫øt qu·∫£ t·ªïng h·ª£p ---
        output = {
            "Predict": label, 
            "Confidence": confidence,
            "Predicted_Line_of_Vulnerability": predicted_line 
        }
        
        with open("fusion_output_agent.json", "w") as f:
            json.dump(output, f, indent=2, ensure_ascii=False)
        print("‚úÖ Saved fusion_output_agent.json")

        # Tr·∫£ v·ªÅ chu·ªói k·∫øt qu·∫£ ƒë∆°n gi·∫£n (b·∫°n ƒë√£ l√†m ƒë√∫ng)
        return f"Successfully predicted vulnerability: {label} (Confidence: {confidence:.2%}). Predicted Line: {predicted_line}. Output saved to fusion_output_agent.json."


# ==== AGENT (C·∫¨P NH·∫¨T) ====
def build_fusion_agent():
    llm_local = LLM(model="ollama/llama3:8b-instruct-q8_0", base_url="http://localhost:11434")
    
    tool = EmbeddingPredictorTool(llm=llm_local)

    agent = Agent(
        role="ML Security Analyzer",
        goal="Analyze embeddings to predict security vulnerabilities and pinpoint the vulnerable code line.",
        backstory="Uses a hybrid approach: an EmbeddingPredictorTool to classify vulnerability types from embeddings, and an LLM to analyze source code and identify the exact line of the predicted vulnerability.",
        tools=[tool],
        verbose=True,
        llm=llm_local,
        # ‚úÖ THAY ƒê·ªîI 2: Th√™m max_iter=1 ƒë·ªÉ CH·∫∂N L·∫∂P V√î H·∫†N
        max_iter=1
    )

    task = Task(
        description=f"Analyze the vulnerability based on the globally loaded 'parser_output.json' and 'contracts/sample.sol'.",
        expected_output="Name of the security vulnerability, its confidence score, and the predicted line number(s) of the vulnerability.",
        agent=agent,
        # ‚úÖ X√≥a 'input' (b·∫°n ƒë√£ l√†m ƒë√∫ng)
    )

    return agent, task

# # Example of how to run it (if needed)
# if __name__ == "__main__":
#     from crewai import Crew, Process
#     agent, task = build_fusion_agent()
#     crew = Crew(
#         agents=[agent],
#         tasks=[task],
#         process=Process.sequential,
#         verbose=True
#     )
#     result = crew.kickoff()
#     print("\n\nFINAL RESULT:")
#     print(result)